{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import geopandas as gpd\n",
    "import fnmatch\n",
    "import os\n",
    "import datetime\n",
    "from datetime import timedelta\n",
    "import glob"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "os.environ['PROJ_LIB'] = 'C:/ProgramData/Miniconda3/Library/share/proj'\n",
    "os.environ['GDAL_DATA'] = 'C:/ProgramData/Miniconda3/Library/share'\n",
    "pyproj_datadir=\"C:\\Anaconda3\\Library\\share\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'init': 'epsg:4326'}"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sd = gpd.read_file('C:/DATA/aus_star_transects/NT_StarTransect_20211111_sj.shp')\n",
    "sd.crs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{}"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sd6 = gpd.read_file('test7.shp')\n",
    "sd6.crs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "sd['date_time'] = sd.obs_time.apply(pd.to_datetime)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['105070' '106069' '102073' '105072' '103076' '104075' '103071' '103072'\n",
      " '102076' '100077' '099078' '101074' '106071' '106072' '107070' '106073'\n",
      " '106070' '105073' '105074' '105075' '105076' '105071' '106068' '104076'\n",
      " '105069' '103078' '103077' '104072' '104071' '103075' '104070' '102078'\n",
      " '102077' '103073' '102075' '103074' '104069' '103070' '104068' '102074'\n",
      " '102072' '101078' '101077' '102071' '101076' '101075' '101073' '101072'\n",
      " '100076' '100075' '101071' '100074' '100073' '100072']\n"
     ]
    }
   ],
   "source": [
    "print (sd.WRSPR.unique())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['Join_Count', 'TARGET_FID', 'publish', 'site', 'obs_time', 'longitude', 'latitude', 'persist', 'crust', 'dist', 'rock', 'green', 'dead', 'litter', 'crypto', 'mid_g', 'mid_d', 'mid_b', 'crn', 'over_g', 'over_d', 'over_b', 'num_points', 'unoccluded', 'obs_key', 'Field24', 'Field25', 'fpc', 'AREA', 'PERIMETER', 'PR_', 'PR_ID', 'RINGS_OK', 'RINGS_NOK', 'PATH', 'ROW', 'MODE', 'SEQUENCE', 'WRSPR', 'PR', 'ACQDayL7', 'ACQDayL8', 'geometry', 'date_time']\n"
     ]
    }
   ],
   "source": [
    "print (list(sd))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sd[\"fwd_date\"] = sd[\"date_time\"] + timedelta(days=30)\n",
    "sd[\"bck_date\"] = sd[\"date_time\"] + timedelta(days=-30)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%run C:\\Users\\grants\\code\\file_management\\list_of_files_multi_dir.py -d Z:/Landsat/wrs2/ -e *dilm[2-3]_zstdmask.img -o test_imglist_dev.csv"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.read_csv('test_imglist_dev.csv',header=None)\n",
    "# get the path row and image date from the image file name\n",
    "df['path_row'] = df[0].map(lambda x: str(x)[-35:-32]) + '_' + df[0].map(lambda x: str(x)[-31:-28])\n",
    "df['img_date'] = df[0].map(lambda x: str(x)[-27:-23]) + '-' + df[0].map(lambda x: str(x)[-23:-21]) + '-' + df[0].map(lambda x: str(x)[-21:-19])\n",
    "df['zone'] = df[0].map(lambda x: str(x)[-14:-13])\n",
    "print (df['zone'].unique())\n",
    "df.to_csv('imglist_proc.csv')\n",
    "\n",
    "\n",
    "#for index, row in df.iterrows():\n",
    "    #print (row[0],row['img_dt'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.read_csv('imglist_proc.csv',header=0)\n",
    "\n",
    "print (list(df))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# read in the shape file and produce a date_time \n",
    "sd = gpd.read_file('C:/DATA/aus_star_transects/NT_StarTransect_20211111_sj.shp')\n",
    "sd['date_time'] = sd.obs_time.apply(pd.to_datetime)\n",
    "\n",
    "\n",
    "# define the number of days either side of the field site measured date to extract stats from the available imagery \n",
    "date_plus = 40 \n",
    "date_minus = -40\n",
    "# create the plus and minus date range \n",
    "sd[\"fwd_date\"] = sd[\"date_time\"] + timedelta(days=date_plus)\n",
    "sd[\"bck_date\"] = sd[\"date_time\"] + timedelta(days=date_minus)\n",
    "\n",
    "\n",
    "# iterrate over the sites in the shapefile and find the image matches and extract out the statistics \n",
    "\n",
    "for index, row in sd.iterrows():\n",
    "    \n",
    "    site_zonal = row\n",
    "    #site_zonal.to_file(\"temp_shp.shp\")\n",
    "    #print (site_zonal.crs)\n",
    "    \n",
    "    site = row['site']\n",
    "    field_date = row['date_time']\n",
    "    \n",
    "    # get the date range for the imagery stats requred\n",
    "    search_date_plus = row['fwd_date']\n",
    "    search_date_minus = row['bck_date']\n",
    "    \n",
    "    # get the info to create the dir path to get a list of imagery \n",
    "    path_row = str(row['PATH']) + '_0' + str(row['ROW'])\n",
    "    year = str(row['obs_time'][:4])\n",
    "    \n",
    "    #search the img list and find matches\n",
    "    df = pd.read_csv('imglist_proc.csv',header=0)\n",
    "    # make the img_date a time date column \n",
    "    df['img_dt'] = df.img_date.apply(pd.to_datetime)\n",
    "        \n",
    "    dfs = df[(df['path_row'] == path_row)]\n",
    "    \n",
    "    \n",
    "    \n",
    "    imgS = dfs[dfs[\"img_dt\"].isin(pd.date_range(search_date_minus,search_date_plus))]\n",
    "    \n",
    "    zone = imgS['zone'].unique()\n",
    "    print (zone)\n",
    "    \n",
    "    #if zone == 2:\n",
    "        #site_zonal.crs \n",
    "    \n",
    "    #print (img_selection.dtypes)\n",
    "        \n",
    "    for row in imgS:\n",
    "        \n",
    "        print ('Site name:',site)\n",
    "        print ('field date:',field_date)\n",
    "        print (path_row)\n",
    "        print ('start search date:',search_date_minus)    \n",
    "    \n",
    "        print (imgS[['img_date','0','zone']])    \n",
    "        print ('end search date:',search_date_plus)\n",
    "    \n",
    "    \n",
    "        print (\"...................\")\n",
    "    \n",
    "    # select out the individual site and reproject it to projected coords UTM 52 or 53 and extract out the stats from \n",
    "    # the given img_selection list using the script - zonal_stats_single_perc.py\n",
    "    \n",
    "    # use geopandas to create the shp file in memory - may need to write it out as a file? if so make it temp and delete...\n",
    "    \n",
    "    # create a temp dir to save the individual results before concating them together \n",
    "    \n",
    "   \n",
    "       \n",
    "    \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df['geometry'] = [Point(xy) for xy in zip(df['Long'], df['Lat'])]\n",
    "# create a df with all of the attributes \n",
    "df2 = df[['geometry','Collectors','Date','Lat', 'Long','photo','Primary','Secondary','Tertiary','lu_code','lu_coden','Commodity','Variety','Management','Confidence','Comments','UIDkey']]\n",
    "# create the geo data frame        \n",
    "df2 = geopandas.GeoDataFrame(df2, geometry='geometry')\n",
    "# set the projection and datum\n",
    "df2.crs = \"+init=epsg:4326\"\n",
    "# save out the geo data frame in esri shapefile format.\n",
    "df2.to_file(shapeName, driver='ESRI Shapefile')  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sds.set_geometry ?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['Join_Count', 'TARGET_FID', 'publish', 'site', 'obs_time', 'longitude', 'latitude', 'persist', 'crust', 'dist', 'rock', 'green', 'dead', 'litter', 'crypto', 'mid_g', 'mid_d', 'mid_b', 'crn', 'over_g', 'over_d', 'over_b', 'num_points', 'unoccluded', 'obs_key', 'Field24', 'Field25', 'fpc', 'AREA', 'PERIMETER', 'PR_', 'PR_ID', 'RINGS_OK', 'RINGS_NOK', 'PATH', 'ROW', 'MODE', 'SEQUENCE', 'WRSPR', 'PR', 'ACQDayL7', 'ACQDayL8', 'geometry']\n",
      "     Join_Count  TARGET_FID publish   site    obs_time   longitude   latitude  \\\n",
      "389           1         389       y  mgp03  2013-05-03  131.711319 -25.910403   \n",
      "\n",
      "     persist  crust  dist  ...  RINGS_NOK  PATH  ROW  MODE  SEQUENCE   WRSPR  \\\n",
      "389    3.333   46.2   0.0  ...          0   103   78     D     25374  103078   \n",
      "\n",
      "         PR  ACQDayL7  ACQDayL8                     geometry  \n",
      "389  103078         8        16  POINT (131.71132 -25.91040)  \n",
      "\n",
      "[1 rows x 43 columns]\n",
      "{'init': 'epsg:4326'}\n",
      "{'init': 'epsg:32752'}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\ProgramData\\Anaconda3\\envs\\raster\\lib\\site-packages\\pyproj\\crs\\crs.py:131: FutureWarning: '+init=<authority>:<code>' syntax is deprecated. '<authority>:<code>' is the preferred initialization method. When making the change, be mindful of axis order changes: https://pyproj4.github.io/pyproj/stable/gotchas.html#axis-order-changes-in-proj-6\n",
      "  in_crs_string = _prepare_from_proj_string(in_crs_string)\n"
     ]
    }
   ],
   "source": [
    "sd = gpd.read_file('C:/DATA/aus_star_transects/NT_StarTransect_20211111_sj.shp')\n",
    "#sd['date_time'] = sd.obs_time.apply(pd.to_datetime)\n",
    "\n",
    "print (list(sd))\n",
    "\n",
    "sda = sd[(sd['site'] == 'mgp03' )]\n",
    "print (sda)\n",
    "\n",
    "#sds = sda[['Join_Count', 'TARGET_FID', 'publish', 'site', 'obs_time', 'longitude', 'latitude', 'persist', 'crust', \n",
    "#          'dist', 'rock', 'green', 'dead', 'litter', 'crypto', 'mid_g', 'mid_d', 'mid_b', 'crn', 'over_g', 'over_d', \n",
    "#          'over_b', 'num_points', 'unoccluded', 'obs_key', 'fpc', 'WRSPR',  'geometry']]\n",
    "\n",
    "sds = gpd.GeoDataFrame(sda, geometry=sda['geometry'])\n",
    "sds.crs  #= \"+init=epsg:4326\" \n",
    "print (sds.crs)\n",
    "#epsg_code_proj4 = '+proj=utm +zone=52 +south +ellps=WGS84 +datum=WGS84 +units=m +no_defs'\n",
    "\n",
    "sdsr = sds.to_crs({'init': 'epsg:32752'})  # (\"EPSG:32752\")      #\n",
    "sdsr.crs \n",
    "print (sdsr.crs)\n",
    "sdsr.to_file('test7.shp', driver='ESRI Shapefile')  \n",
    "\n",
    "#print (sda)\n",
    "\n",
    "\n",
    "\n",
    "#df['geometry'] = [Point(xy) for xy in zip(df['Long'], df['Lat'])]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "    if epsg_code == 32752:\n",
    "        epsg_code_proj4 = '+proj=utm +zone=52 +south +ellps=WGS84 +datum=WGS84 +units=m +no_defs'\n",
    "    \n",
    "    if epsg_code == 32753:\n",
    "        epsg_code_proj4 = '+proj=utm +zone=53 +south +ellps=WGS84 +datum=WGS84 +units=m +no_defs'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "ax = sdsr.plot()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%run C:\\Users\\gwsta\\code\\zonal\\zonal_stats_single_perc.py -h"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "http://localhost:8888/edit/code/zonal/zonal_stats_single_perc.py"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "    df = pd.read_csv(csvName, header=0)\n",
    "    \n",
    "    df['geometry'] = [Point(xy) for xy in zip(df['Long'], df['Lat'])]\n",
    "    # create a df with all of the attributes \n",
    "    df2 = df[['geometry','Collectors','Date','Lat', 'Long','photo','Primary','Secondary','Tertiary','lu_code','lu_coden','Commodity','Variety','Management','Confidence','Comments','UIDkey']]\n",
    "    # create the geo data frame        \n",
    "    df2 = geopandas.GeoDataFrame(df2, geometry='geometry')\n",
    "    # set the projection and datum\n",
    "    df2.crs = \"+init=epsg:4326\"\n",
    "    # save out the geo data frame in esri shapefile format.\n",
    "    df2.to_file(shapeName, driver='ESRI Shapefile') "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "site_list=[]\n",
    "site_date=[]\n",
    "\n",
    "\n",
    "#for pathrow in sd.WRSPR.unique():\n",
    "    \n",
    "for index, row in sd.iterrows():\n",
    "    \n",
    "    site = row['site']\n",
    "    field_date = row['date_time']\n",
    "    \n",
    "    # get the date range for the imagery stats requred\n",
    "    search_date_plus = row['fwd_date']\n",
    "    search_date_minus = row['bck_date']\n",
    "    \n",
    "    # get the info to create the dir path to get a list of imagery \n",
    "    path_row = str(row['PATH']) + '_0' + str(row['ROW'])\n",
    "    year = str(row['obs_time'][:4])\n",
    "    \n",
    "    #month = row['obs_time'][5:7]\n",
    "    \n",
    "    print (path_row)\n",
    "    \n",
    "    print (search_date_minus)\n",
    "    \n",
    "    print (field_date)\n",
    "    \n",
    "    print (year)\n",
    "        \n",
    "    print (search_date_plus)\n",
    "    \n",
    "    print (\"...................\")\n",
    "    \n",
    "    dirname = 'C:/data/landsat/' + path_row + '/' + year + '/'\n",
    "    \n",
    "    print (dirname)\n",
    "    print (\"...................\")\n",
    "    \n",
    "    pattern = '_zstdmask.img'\n",
    "       \n",
    "    \n",
    "    img_list = []\n",
    "    \n",
    "    \n",
    "    for root, dirs, files in os.walk(dirname):\n",
    "        for file in files:\n",
    "            if file.endswith(pattern):\n",
    "                img = (os.path.join(root, file))\n",
    "                img_list.append(img)\n",
    "                #print (img)\n",
    "    \n",
    "    print (img_list)\n",
    "    \n",
    "    df = pd.DataFrame(img_list)\n",
    "    \n",
    "    \n",
    "\n",
    "    #for img in img_list:\n",
    "        \n",
    "        #date = img[-27:-19]\n",
    "        \n",
    "        #if date > search_date_mimus & < search_date_minus: \n",
    "             \n",
    "          \n",
    "\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dirname = 'D:/landsat/105_070/2011/'\n",
    "    \n",
    "print (dirname)\n",
    "print (\"...................\")\n",
    "    \n",
    "pattern = '_zstdmask.img'\n",
    "       \n",
    "    \n",
    "list_img = []\n",
    "    \n",
    "    \n",
    "for root, dirs, files in os.walk(dirname):\n",
    "    for file in files:\n",
    "        if file.endswith(pattern):\n",
    "            img = (os.path.join(root, file))\n",
    "            list_img.append(img)\n",
    "            print (img)\n",
    "    \n",
    "print (imglist)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "site_list=[]\n",
    "site_date=[]\n",
    "\n",
    "\n",
    "#for pathrow in sd.WRSPR.unique():\n",
    "    \n",
    "for index, row in sd.iterrows():\n",
    "    \n",
    "    site = row['site']\n",
    "    date = row['date_time']\n",
    "    year = row['obs_time'][:4]\n",
    "    month = row['obs_time'][5:7]\n",
    "    \n",
    "    month_m1 = int(month)-1\n",
    "    month_p1 = int(month)+1\n",
    "    \n",
    "    print (month_m1)\n",
    "    print (month_p1)\n",
    "    \n",
    "    if month_p1 <=9:\n",
    "        \n",
    "        month_p1 = str(month_p1)[0:3] + '0' + str(month_p1)\n",
    "        print (\"...................\")\n",
    "        print (month_p1)\n",
    "        print (\"...................\")\n",
    "    if month_m1 <=9:   \n",
    "        month_m1 = str(month_m1)[0:3] + '0' + str(month_m1)\n",
    "        \n",
    "    yearmonth = str(row['obs_time'][:4])+str(row['obs_time'][5:7])\n",
    "    \n",
    "    yearmonth_p1 = str(row['obs_time'][:4]+str(month_p1))\n",
    "    yearmonth_m1 = str(row['obs_time'][:4]+str(month_m1))\n",
    "    \n",
    "    path_row = str(row['PATH']) + '_0' + str(row['ROW'])\n",
    "    \n",
    "    #print (site, ' ', date, ' ', year, ' ',month, ' ', path_row)\n",
    "    print (site, ' ', date) \n",
    "    print ('plus month ', yearmonth_p1)\n",
    "    print ('minus month ', yearmonth_m1)\n",
    "    # select the folders to create the image list for each site\n",
    "    \n",
    "    # construct the file path from the site information \n",
    "    seach_month = [yearmonth_m1,yearmonth,yearmonth_p1]\n",
    "    \n",
    "    for month in seach_month:\n",
    "        \n",
    "            \n",
    "        dirname = 'Z:/Landsat/wrs2/' + path_row + '/' + year + '/' + month + '/'\n",
    "    \n",
    "        print (dirname)\n",
    "    \n",
    "        pattern = '*_zstdmask.img'\n",
    "    \n",
    "        #imglist = fnmatch.filter(os.listdir(dirname), pattern)\n",
    "    \n",
    "        #print (imglist)\n",
    "    \n",
    "    "
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
